Очистка данных (Data cleaning|Data cleansing|Data Preprocessing) - это удаление пропущенных значений, дубликатов и нерелевантных данных. Целью очистки данных является достижение точных, согласованных и свободных от ошибок данных.
#### Цикл очистки данных:
- Удаление нежелательных наблюдений - Removal of unwanted observations
- Исправление структурных ошибок - Fixing Structural errors
- Управление нежелательных выбросов - Managing Unwanted outliers
- Обработка пропущенных данных - Handling missing data
### Основные этапы
1. Обозревание данных и их исследование - понимание данных посредством их осмотра и обнаружения пропущенных значений, выбросов и некорректных данных.
- Проверка на дубликаты:  `df.duplicated()`
- Обзор признаков на количество пропущенных значений и типов данных: `df.info()`. Здесь можно выяснить, например: Некоторые признаки имеют ...
	- null значения,
	- категориальный тип данных (object). Всё это требует обработки.
- Статистический анализ: `df.describe()`: count, mean, std, min, 25%, 50%, 75%, max.
- Сохранение категориальных и числовых признаков:
```python
# Categorical columns
cat_col = [col for col in df.columns if df[col].dtype == 'object']
print('Categorical columns :',cat_col)
# Numerical columns
num_col = [col for col in df.columns if df[col].dtype != 'object']
print('Numerical columns :',num_col)
```
- Вывод мощности (количества уникальных значений) категориальных признаков: `df[cat_col].nunique()`
2. Удаление нежелательных наблюдений - дубликатов, избыточных или некорректных значений. Дубликаты чаще всего возникают в процессе сбора данных. Некорректные значения - те, которые не подходят для решения конкретно заданной проблемы
- Избыточные наблюдения снижают эффективность модели тем, что они могут склонять в ту или иную сторону, что влияет на корректность результата.
- Нерелевантные наблюдения - те, которые являются неинформативными для поставленной задачи и могут быть удалены непосредственно.
- Проектирование признаков (Feature Engineering) - получение новых признаков из одного или группы существующих признаков.
3. Обработка пропущенных значений. Пропущенные значения могут возникать из-за человеческого фактора, системных ошибок или проблем во время сбора данных. 
- Техники, используемые для обработки пропущенных значений: __вменение(условный расчёт), удаление или замена.__ 
	- Для просмотра процента пропущенных значений используется формула: `round((df.isnull().sum()/df.shape[0])*100, 2)`
	- Далее пропущенные значения нужно осторожно обработать, так как они могут обозначать что-то важное.
		- Тот факт, что значение отсутствует, может быть информативным само по себе
		- Часто есть необходимость в предсказании на новых данных, даже если некоторые из признаков отсутствуют.
	- Однако если пропущенных значений у некоторого признака достаточно много, то признак следует удалить (>60%)
	- И если пропущенных значений в некотором признаке достаточно мало (<1%), то удаляют наблюдения.
```python
df2 = df1.drop(columns='Cabin')
df2.dropna(subset=['Embarked'], axis=0, inplace=True)
df2.shape
```
- Условный расчёт пропущенных значений на основе предыдущих наблюдений:
	- Пропущенные значения почти всегда информативны сами по себе, поэтому следует сказать алгоритму, что значения были пропущены.
	- Даже если модель будет рассчитывать пропущенные значения, это не добавит реальной информации, а только усилит паттерны, которые и так существовали за счёт других признаков.
	- Если у некоторого признака маленькая разница между средней и медианой, то тогда используют условный расчёт на среднее значение или медиану:
		- Расчёт на среднее - подходит если признак находится в нормальном распределении (mean=0, std=1) и не содержит экстремальных выбросов.
		- Расчёт на медианное - подходит, когда данные содержат выбросы или перекосы. 
```python
# Mean imputation
df3 = df2.fillna(df2.Age.mean())
# Let's check the null values again
df3.isnull().sum()

```
4. Обработка выбросов. 
- Выбросы - это экстремальные значения, которые значительно отклоняются от большинства данных. Они могут негативно влиять на анализ и производительность модели. Техники по обработке выбросов: кластеризация, интерполяция, преобразования.
- Инструмент обнаружения выбросов и перекосов в данных - прямоугольный график/короб с усами(box plot|box-and-whisker plot) - является графическим представлением распределения набора данных, включая медианное (линия внутри прямоугольника), квартили (границы прямоугольника - межквартильный диапазон - interquartile range - IQR) и возможные выбросы (точки за пределами усов). Усы - прямые за пределами прямоугольника, растянувшиеся в 1.5 раз межквартильного диапазона по обе стороны от медианы, означают невыбросные наблюдения.
```python
import matplotlib.pyplot as plt

plt.boxplot(df3['Age'], vert=False)
plt.ylabel('Variable')
plt.xlabel('Age')
plt.title('Box Plot')
plt.show()

```
- Удаление выбросов - наблюдения слева и справа от усов:
```python
# calculate summary statistics
mean = df3['Age'].mean()
std = df3['Age'].std()

# Calculate the lower and upper bounds
lower_bound = mean - std*2
upper_bound = mean + std*2

print('Lower Bound :',lower_bound)
print('Upper Bound :',upper_bound)

# Drop the outliers
df4 = df3[(df3['Age'] >= lower_bound)
		& (df3['Age'] <= upper_bound)]
```
5.0 Валидация и верификация данных 
- необходима для того, чтобы гарантировать, что данные точны и согласованы. Для этого их сравнивают с внешними источниками или экспертными знаниями.
- Данные разделяют на независимые и целевые признаки: 
```python
X = df3[['Pclass','Sex','Age', 'SibSp','Parch','Fare','Embarked']]
Y = df3['Survived']
```
5. [[Масштабирование признаков - Feature Scaling(Engineering)]]

#### Инструменты для очистки данных
- OpenRefine
- Trifacta Wrangler 
- TIBCO Clarity
- Cloudingo
- IBM Infosphere Quality Stage