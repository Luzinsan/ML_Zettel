Imbalanced Data Distribution - возникает, когда количество наблюдений для какого-то класса намного больше или меньше других классов. Так как алгоритмы машинного обучения стремятся повысить точность путем уменьшения погрешности, они не рассматривают распределение классов. Это проблема распространена, например: обнаружение мошенничества, обнаружение аномалий, распознавание лиц.
Стандартные техники (Дерево решений, логистическая регрессия), имеют предубеждения к мажорным класса, тогда как минорные классы они склонны игнорировать. Тогда возникает большая ошибка в классификации минорных классов, так как минорные данные хуже запоминаются и выделяются (lesser recall).
Основные техники обработки несбалансированных данных:
1. SMOTE
2. Near Miss Algorithm
# SMOTE - Synthetic Minority Oversampling Technique - Oversampling 
# Метод Передискретизации Синтетического Меньшинства - Передискретизация
- Нацелен на балансирование распределение классов путем рандомного увеличения наблюдений для минорных классов, тиражируя их. 
- SMOTE синтезирует новые минорные экземпляры между уже существующими минорными экземплярами - другими словами - генерирует виртуальные тренировочные записи для минорных классов с помощью интерполяции.
- Синтетические тренировочные записи генерируются путем рандомного выбора одного или нескольких k-ближайших соседей (k-nearest neighbors) для каждого наблюдения в минорном классе.
- После процесса передискретизации, данные восстанавливаются и некоторые модели классификации могут быть применены для обработки данных.
## Поэтапное объяснение SMOTE
1. Пусть имеется минорный класс A и каждое наблюдение $x \in A$. Высчитываем посредством Евклидова расстояния (Euclidean distance) между наблюдением x и остальными наблюдениями из класса A расстояние между наблюдениями.
2. Частота дискретизации N (sampling rate) устанавливается в соответствии с пропорцией несбалансированности классов. Для каждого наблюдения $x \in A$ выбирается N ближайших соседей с помощью алгоритма k-ближайших соседей. Так составляется $A_1$ подмножество, включенное в множество $A$.
3. Для каждого соседа $x_k \in A_1$ ($k=1,2,3,..,N$) генерируется новое наблюдение по формуле    $$x' = x + rand(0,1)*|x - x_k|$$
   # NearMiss Algorithm - Undersampling - методика исключения
   - Это методика недостаточного отбора наблюдений, которая нацелена на балансировку распределения классов посредством рандомного исключения наблюдений мажорных классов.
   - Когда наблюдения двух различных классов близки друг к другу, мы можем удалить наблюдения мажорного класса так, чтобы увеличить пространство между двумя классами. 
   - Для того чтобы избежать потерю информации, используют метод ближайших соседей
   ## Основополагающие шаги работы алгоритма ближайших соседей
   1. Метод находит все расстояния между наблюдениями мажорного и минорного класса. Для данной задаче только мажорный класс будет подвержен выборкой с недостатком (NearMiss)
   2. Далее выбираются n наблюдений из мажорного класса такие, которые имеют наименьшее расстояние до наблюдений минорного класса.
   3. Если имеется k наблюдений минорного класса, то алгоритм ближайших соседей возвратит $k*n$ наблюдений мажорного класса.
   ### Существуют различные вариации NearMiss алгоритма в зависимости от метода нахождения ближайших наблюдений:
   1. Выборка экземпляров мажорного класса, которые имеют наименьшее среднее расстояние до k __ближайших__ соседей минорного класса.
   2. Выборка экземпляров мажорного класса, которые имеют наименьшее расстояние до k __наиболее далёких__ экземпляров минорного класса.
   3. Для каждого экземпляра минорного класса сохраняется их M ближайших соседей. Затем выбираются экземпляры мажорного класса, для которых среднее расстояние до N ближайших соседей является наибольшим.
   # На примере: https://www.kaggle.com/mlg-ulb/creditcardfraud
   ```python
# import necessary modules 
import pandas  as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, classification_report
  
# load the data set
data = pd.read_csv('creditcard.csv')
  
# print info about columns in the dataframe
print(data.info())
```
   ![[Pasted image 20230808115946.png]]
```python
# normalise the amount column
data['normAmount'] = StandardScaler().fit_transform(np.array(data['Amount']).reshape(-1, 1))

# drop Time and Amount columns as they are not relevant for prediction purpose
data = data.drop(['Time', 'Amount'], axis = 1)

# as you can see there are 492 fraud transactions.
data['Class'].value_counts()
```
- ![[Pasted image 20230808120026.png]]
- Разделение на тренировочную и тестовую выборку:
```python
from sklearn.model_selection import train_test_split

# split into 70:30 ration
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)

# describes info about train and test set
print("Number transactions X_train dataset: ", X_train.shape)
print("Number transactions y_train dataset: ", y_train.shape)
print("Number transactions X_test dataset: ", X_test.shape)
print("Number transactions y_test dataset: ", y_test.shape)
```
- ![[Pasted image 20230808120119.png]]
- Тренировка модели без балансировки распределения классов:
```python
# logistic regression object
lr = LogisticRegression()

# train the model on train set
lr.fit(X_train, y_train.ravel())

predictions = lr.predict(X_test)

# print classification report
print(classification_report(y_test, predictions))
```
- ![[Pasted image 20230808120215.png]]
	- Отклик (recall) класса 1 = 62%, что означает, что модель склонна к мажорному классу.
## [[Практика использования SMOTE]]
## [[Практика использования NearMiss]]