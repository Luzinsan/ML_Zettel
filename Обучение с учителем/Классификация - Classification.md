#SupervisedLearning 

---
- Классификация это задача интеллектуального анализа ([[Data Mining - Интеллектуальный анализ данных]]), которая включает назначение класса меток каждому наблюдению в датасете, основываясь на их признаках.  Цель классификации - построить модель, которая точно предсказывает класс меток новым наблюдениям.
- Входные данные разделяются на два и более класса, а по итогу обучения должна получиться модель, которая назначает невидимые входные данные одному или нескольким (в случае многофакторной классификации) из этих классов. Далее предсказывает, принадлежит или не принадлежит вход определённому классу. 
- Модели классификации подразделяются на две группы:
  ![[Pasted image 20230808131617.png]]
	- Бинарная классификация (Binary classification)
		- Фильтрация спама: входные данные: почта или сообщения. Модель классифицирует их как "спам" или "не спам"
	- Мультиклассовая Классификация (Multiclass Classification)

# Типы обучения в алгоритмах классификации (Типы классификаторов)
- **Пассивное (Lazy Learners)** - обучение, основанное на наблюдениях - не обучается во время тренировочной фазы, вместо этого оно просто сохраняет тренировочные данные и использует их для классификации новых наблюдений во время предсказания.
  Другой термин, обозначающий то же самое:
  **Описательный (Discriminative) классификатор** - это очень простой классификатор, который определяет только один класс для каждой строки данных. Он пытается моделировать, просто полагаясь на наблюдаемые данные, в значительной степени зависит от качества данных, а не от распределения.
	- [[Логистическая регрессия - Logistic Regression]]
	- Очень быстрое за время прогнозирования, потому что не требует вычислений.
	- Менее эффективное для многомерных пространств или когда количество наблюдений велико.
	- Примеры: [[K-Nearest Neighbors - Алгоритм K-ближайших соседей]] и рассуждения, основанные на наблюдениях (case-based reasoning)
- **Активное (Eager Learners)** - обучение, основанное на моделях - обучение модели на основе тренировочных данных и использование модели для классификации новых наблюдений во время предсказания.
   Другой термин, обозначающий то же самое:
   **Генеративный (Generative) классификатор** - моделирует распределение отдельных классов и пытается научить модель, которая генерирует данные по предпосылкам, оценивая допущения и распределение модели. Используется для предсказания новых данных.
	- Когда в модель поступает новое наблюдение, она должна __предсказать__ объект некоторой меткой. 
	- Наиболее эффективное для многомерных пространств, имеющих большой тренировочный датасет.
	- Примеры: [[Decision Tree - Дерево решений]], [[Random Forest - Рандомный лес]], [[Naive Bayes - Наивный Байес]], метод опорных векторов.

## [[Оценка модели классификации]]

## Как работает классификация
![[Pasted image 20230815164429.png]]
Основная идея - обучить модель на размеченном датасете, где входные данные ассоциированы с их соответствующими выходными метками, чтобы научить паттернам и взаимосвязям между входными данными и выходными метками. Как только модель обучится, её можно использовать для предсказания выходных меток на новых данных.
Классификация основана на построении функции, получающей на вход вектор "X" и предсказываемых выходов "Y" (качественного ответа на подаваемые значения входного множества).
Используемый классификатор (модель) - это контролируемая функция (supervised), которая может быть создана вручную, основываясь на экспертных знаниях.
1. __Понимание проблемы__. До начала классификации, важно понять проблему, которую вы пытаетесь решить. Что за класс меток вы пытаетесь предсказать? Какие есть взаимосвязи между входными данными и классом метод. Пример:
	- Мы должны предсказать, имеет ли пациент заболевание, или нет, на основе 7 независимых признаков (фич). Это означает, что есть только 2 возможных выхода: True - пациент имеет заболевание, False - заболевания нет.
2. __Подготовка данных__. Включает: 
	- сбор: собираются данные, релевантные для решаемой проблемы, и содержащие все необходимые атрибуты и метки данных для классификации. Могут собираться из различных источников: исследования, опросники, сайты и базы данных (surveys, questionnaires, websites, and databases);
	- предобработку данных: обработка пропущенных значений (замена на среднее, медиану или моду соответствующего признака, либо удаление таких записей), работа с выбросами (обнаружение: z-score анализ, boxplot, scatterplot; могут быть заменены на среднее, медиану или моду, либо удалены из датасета), преобразование в формат, подходящий для анализа (масштабирование или нормализация, для гарантии того, что все признаки имеют единый масштаб); [[Конвертирование данных]] в числовую форму (многие алгоритмы классификации поддерживают только такой вход);
	- разделение их на тренировочное, валидационное и тестовое множества. На этом шаге данные уже очищены, предобработаны и преобразованы в формат, которых может быть использован для алгоритма классификации.
		- X - это независимые признаки в виде матрицы $N*M$, где N - это кол-во наблюдений, M - кол-во признаков (фич) 
		- y - вектор соответствующих предсказательных классов для каждого из N наблюдений.
3. __Извлечение признаков__. Релевантные признаки извлекаются из данных, которые могут быть использованы для разделения на различные классы.
	- Используемые техники: анализ корреляции (обнаружение корреляций между признаками в датасете -> признак, который сильно коррелирует с другим, следует удалять, так как он не предоставляет дополнительной информации для модели классификации), получение информации (information gain) (мера количества информации признака, предоставленного для классификации -> признаки с высокой информационной значимостью выбираются для модели классификации), анализ основных компонентов (principal component analysis - PCA) (используется для снижения размерности датасета -> идентифицирует наиболее важные признаки в датасете и удаляет избыточные).
	- Предположим, наши входные данные X имеют 7 независимых признаков, где только 5 признаков влияют на метки классов, а оставшиеся 2 - не коррелируют, поэтому мы будем использовать только эти 5 признаков для тренировки модели.
4. __Выбор модели (Model Selection)__. Есть множество различных моделей, которые могут быть использованы для классификации, включая логистическую регрессию, [[Decision Tree - Дерево решений]], метод опорных векторов (SVM) ([[Support Vector Machines having kernel = ‘linear’ - Метод опорных векторов с линейным ядром]]), или нейронные сети ([[Multi-layer Artificial Neural Network - Многослойная искусственная нейронная сеть]]). Важно выбрать модель, которая соответствует вашей проблеме, принимая во внимание размер и сложность данных, а также доступные вычислительные ресурсы.
5. __Тренировка модели__. Помимо самой тренировки на тренировочных данных, этот этап включает настройку параметров модели для минимизации ошибок между классом предсказываемых меток и классов фактических меток для тренировочных данных.
6. __Оценка модели__. ([[Оценка модели классификации]]) Оценка производительности на валидационном множестве. Она даёт представление о том, как хорошо модель работает на новых данных.
    ![[Pasted image 20230816125048.png]]
	- Логистическая потеря или кросс-энтропийная потеря (Log Loss or Cross-Entropy Loss), матрица смещения (Confusion Matrix), точность, отзывчивость и AUC-ROC кривые - качественные метрики, используемые для измерения производительности модели.
7. __Точная настройка модели (Fine-tuning the model)__. Если производительность модель неудовлетворительна, то её параметры можно ещё настроить или выбрать другую модель.
8. __Развертывание модели (Deploying the model)__. Как только все требования удовлетворены, мы можем задеплоить её для выполнения предсказаний на новых данных, уже из реального мира.